{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is Jennifer's first pass at the data to do EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### EDA and preprocesssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:52:59.535551Z",
     "start_time": "2018-11-23T00:52:58.889999Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "spotify_color = '#5bb560'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:00.133323Z",
     "start_time": "2018-11-23T00:52:59.537748Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file_path = ('data/track_features/tf_mini.csv')\n",
    "track_features = pd.read_csv(file_path)\n",
    "track_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:00.196703Z",
     "start_time": "2018-11-23T00:53:00.137363Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "track_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:00.782397Z",
     "start_time": "2018-11-23T00:53:00.200376Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_mini_path = 'data/training_set/log_mini.csv'\n",
    "log_mini = pd.read_csv(log_mini_path)\n",
    "log_mini.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:00.793812Z",
     "start_time": "2018-11-23T00:53:00.785246Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# what is the distribution of skipping\n",
    "\n",
    "# plt.hist((log_mini.groupby('session_id')['skip_1'].sum()/log_mini.groupby('session_id')['session_length'].max()), bins = 20, alpha = 0.5, color = 'g')\n",
    "# plt.hist((log_mini.groupby('session_id')['skip_2'].sum()/log_mini.groupby('session_id')['session_length'].max()), bins = 20, alpha = 0.5, color = 'blue')\n",
    "# plt.hist((log_mini.groupby('session_id')['skip_3'].sum()/log_mini.groupby('session_id')['session_length'].max()), bins = 20, alpha = 0.5, color = 'purple')\n",
    "\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:01.042827Z",
     "start_time": "2018-11-23T00:53:00.798935Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.hist((log_mini.groupby('session_id')['not_skipped'].sum()/log_mini.groupby('session_id')['session_length'].max()) , bins = 20)\n",
    "plt.title('Distribution of not_skip rates per session')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:01.113691Z",
     "start_time": "2018-11-23T00:53:01.045510Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.mean(log_mini.groupby('session_id')['not_skipped'].sum()/log_mini.groupby('session_id')['session_length'].max())\n",
    "\n",
    "# this tells me that on average, people are listening to a third of the session to completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:01.123070Z",
     "start_time": "2018-11-23T00:53:01.117498Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# log_mini.session_length.unique() # this tells me that I need to normalize the skip rate with the session length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:01.288652Z",
     "start_time": "2018-11-23T00:53:01.127310Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_mini.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:01.781970Z",
     "start_time": "2018-11-23T00:53:01.299286Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# I would be interested in turning the date into a datetime object and then \n",
    "# extracting the weekday to see if there is some seasonality happening\n",
    "from datetime import datetime as dt\n",
    "\n",
    "log_mini.date = pd.to_datetime(log_mini.date)\n",
    "log_mini['weekday'] = log_mini.date.dt.dayofweek # Return the day of the week as an integer, where Monday is 0 and Sunday is 6\n",
    "\n",
    "# this also means that I'm going to need to get dummy variables from weekday since it's categorical\n",
    "log_mini = pd.concat([log_mini, pd.get_dummies(log_mini['weekday'], prefix = 'weekday', drop_first = True)], axis = 1)\n",
    "log_mini = pd.concat([log_mini, pd.get_dummies(log_mini['hour_of_day'], prefix = 'hour', drop_first = True)], axis = 1)\n",
    "log_mini = pd.concat([log_mini, pd.get_dummies(log_mini['session_position'], prefix = 'sess_pos', drop_first = True)], axis = 1)\n",
    "log_mini = pd.concat([log_mini, pd.get_dummies(log_mini['hist_user_behavior_reason_start'], prefix = 'start_hist_b', drop_first = True)], axis = 1)\n",
    "log_mini = pd.concat([log_mini, pd.get_dummies(log_mini['hist_user_behavior_reason_end'], prefix = 'end_hist_b', drop_first = True)], axis = 1)\n",
    "log_mini = pd.concat([log_mini, pd.get_dummies(log_mini['context_type'], prefix = 'context', drop_first = True)], axis = 1)\n",
    "\n",
    "\n",
    "log_mini.drop(['weekday', 'hour_of_day', 'session_position', 'context_type', 'date','track_id_clean', 'track_id',\n",
    "               'session_id','short_pause_before_play', 'long_pause_before_play',\n",
    "               'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end'], axis = 1, inplace = True, errors = 'ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Upon initial reading, I thought the problem was a classification problem in which we're asked to predict whether a user will skip this song or not (binary outcome). Now that I look at the df some more, I see that there are varying degrees of 'skip'. I think I'm going to try to simplify this problem first, and delve deeper when I've made modelling progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "let's first get familiar with the data by looking at the data for one session log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:01.787966Z",
     "start_time": "2018-11-23T00:53:01.783726Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# session_log_mask = (log_mini.session_id == log_mini.session_id[0])\n",
    "# one_session = log_mini[session_log_mask]\n",
    "# one_session_detailed = pd.merge(one_session, track_features, how = 'left', left_on='track_id_clean', right_on = 'track_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:01.809566Z",
     "start_time": "2018-11-23T00:53:01.801357Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# to simplify this problem I only care about if they played the entire song or not (skip def)\n",
    "# one_session_detailed.drop(['skip_1', 'skip_2', 'skip_3', 'session_id', 'track_id_clean', 'track_id'], axis = 1, \n",
    "#                           inplace = True, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:01.824109Z",
     "start_time": "2018-11-23T00:53:01.814054Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one_session_detailed.info()\n",
    "# my target feature is 'not_skipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:01.840114Z",
     "start_time": "2018-11-23T00:53:01.831122Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# i want to see the distribution of songs that play all the way through\n",
    "\n",
    "\n",
    "# plt.hist(log_mini.groupby('session_id')['not_skipped'].sum(), bins = 20)\n",
    "# plt.title('Distribution of songs that were not skipped per session')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:02.120662Z",
     "start_time": "2018-11-23T00:53:01.846993Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I want to make sure to compare it against a dummy classifier so i can establish a baseline\n",
    "# i need to scale things\n",
    "\n",
    "log_mini.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling without dummifying\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Justification for starting with RandomForest:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:02.126780Z",
     "start_time": "2018-11-23T00:53:02.123529Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# log_mini.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:41.326271Z",
     "start_time": "2018-11-23T00:53:40.328472Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Feature ranking with recursive feature elimination.\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "log_mini_no_process = pd.read_csv('data/training_set/log_mini.csv')\n",
    "skipped_data = log_mini_no_process[['track_id_clean', 'not_skipped']].copy()\n",
    "\n",
    "track_features['is_major'] = (track_features['mode'] == 'major').astype(int)\n",
    "session_with_track_info = (pd.merge(log_mini_no_process, track_features, left_on='track_id_clean', right_on='track_id')\n",
    "                            .drop(['track_id_clean', 'track_id'], axis=1))\n",
    "session_with_track_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:50.255825Z",
     "start_time": "2018-11-23T00:53:43.736046Z"
    }
   },
   "outputs": [],
   "source": [
    "# grid_params = {\n",
    "#     'n_estimators': [10, 50],\n",
    "#     'max_depth': [3, 4, 6]\n",
    "# }\n",
    "\n",
    "rf_model = RandomForestClassifier() #GridSearchCV(RandomForestClassifier(), params, cv = 5)\n",
    "\n",
    "features = session_with_track_info.drop(['not_skipped', 'date', 'context_type','hist_user_behavior_reason_start', 'mode', 'hist_user_behavior_reason_end', 'session_id'], axis=1)\n",
    "target = log_mini_no_process.not_skipped\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target)\n",
    "rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trying with only metadata to predict skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:54.771692Z",
     "start_time": "2018-11-23T00:53:54.026861Z"
    }
   },
   "outputs": [],
   "source": [
    "track_features_for_skipped = pd.read_csv('data/track_features/tf_mini.csv')\n",
    "songs_skipping = (pd.merge(skipped_data, track_features, how='left', left_on='track_id_clean', right_on='track_id')\n",
    "                  )\n",
    "songs_skipping.drop(columns=['track_id_clean','mode'], inplace=True)\n",
    "# songs_skipping['is_major'] = \n",
    "# songs_skipping.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:59.269724Z",
     "start_time": "2018-11-23T00:53:56.160289Z"
    }
   },
   "outputs": [],
   "source": [
    "features = songs_skipping.drop(columns=['not_skipped', 'track_id']).copy(deep=True)\n",
    "target = songs_skipping.not_skipped\n",
    "\n",
    "sm = SMOTE(random_state= 42)\n",
    "x_resampled, y_resampled = sm.fit_sample(features, target)\n",
    "\n",
    "with open('smote_data.pkl', 'wb') as file:\n",
    "    pickle.dump([features, target], file)\n",
    "    file.close()\n",
    "    \n",
    "print('I have loaded the smoted data into a pickle file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:54:00.814089Z",
     "start_time": "2018-11-23T00:54:00.785206Z"
    }
   },
   "outputs": [],
   "source": [
    "x_resampled = pd.DataFrame(x_resampled, columns=features.columns)\n",
    "x_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:54:16.261345Z",
     "start_time": "2018-11-23T00:54:09.455569Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled)\n",
    "songs_rf_model = RandomForestClassifier()\n",
    "songs_rf_model.fit(x_train, y_train)\n",
    "predictions = songs_rf_model.predict(x_test)\n",
    "print(accuracy_score(predictions, y_test))\n",
    "print(classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:54:26.804591Z",
     "start_time": "2018-11-23T00:54:26.780269Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(zip(songs_rf_model.feature_importances_, features.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:53:04.284884Z",
     "start_time": "2018-11-23T00:52:58.942Z"
    }
   },
   "outputs": [],
   "source": [
    "# rf_model.score\n",
    "predictions = songs_rf_model.predict(x_test)\n",
    "print(classification_report(predictions, y_test))\n",
    "print(confusion_matrix(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional EDA to understand the data for Analytics v Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:54:47.365976Z",
     "start_time": "2018-11-23T00:54:47.201022Z"
    }
   },
   "outputs": [],
   "source": [
    "# i would be interested to compare the songs skipped based on the is_major column\n",
    "major_df = pd.DataFrame(songs_skipping.groupby('is_major')['not_skipped'].sum())\n",
    "plt.bar(major_df.index, major_df.not_skipped, color=spotify_color)\n",
    "plt.title('Number of Skipped in Dataset')\n",
    "plt.xticks(major_df.index, ['skipped', 'not_skipped'])\n",
    "plt.savefig('figures/skipped_song_hist.svg', format='svg')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:46:02.928933Z",
     "start_time": "2018-11-23T00:46:02.894829Z"
    }
   },
   "outputs": [],
   "source": [
    "# i'm interested in how the rate of skips changes with each listening hour\n",
    "\n",
    "songs_skipping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:56:32.603773Z",
     "start_time": "2018-11-23T00:56:13.682816Z"
    }
   },
   "outputs": [],
   "source": [
    "session_dates = log_mini_no_process[['track_id_clean', 'date', 'hour_of_day']].copy()\n",
    "session_dates.head()\n",
    "songs_dates = pd.merge(songs_skipping, session_dates, how='left', left_on='track_id', right_on='track_id_clean')\n",
    "songs_dates.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T01:11:07.046510Z",
     "start_time": "2018-11-23T01:10:49.298840Z"
    }
   },
   "outputs": [],
   "source": [
    "songs_dates.groupby('hour_of_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-23T01:11:25.302Z"
    }
   },
   "outputs": [],
   "source": [
    "len(log_mini_no_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:55:18.579394Z",
     "start_time": "2018-11-23T00:55:18.562394Z"
    }
   },
   "outputs": [],
   "source": [
    "log_mini_no_process.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:08:26.173423Z",
     "start_time": "2018-11-22T20:08:26.153055Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(zip(songs_rf_model.feature_importances_, features.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:07:53.915808Z",
     "start_time": "2018-11-22T20:07:35.849Z"
    }
   },
   "outputs": [],
   "source": [
    "# do you know what this is plotting\n",
    "plt.plot(log_mini_no_process.groupby('session_position')['not_skipped'].sum())\n",
    "plt.xticks(range(1,21));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling (Jenn Wong original approach)\n",
    "\n",
    "Recall, we are trying to predict 'not_skipped' aka played the entire song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:07:53.987631Z",
     "start_time": "2018-11-22T20:07:35.866Z"
    }
   },
   "outputs": [],
   "source": [
    "x_features = log_mini.drop(['not_skipped'], axis = 1)\n",
    "y_target = log_mini.not_skipped\n",
    "\n",
    "# frmo experience running before scaling, scaling doesn't change the metrics but doing it for good measure\n",
    "scaler = StandardScaler()\n",
    "x_transformed = scaler.fit_transform(x_features)\n",
    "x_features = pd.DataFrame(x_transformed, columns = x_features.columns)\n",
    "x_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:07:53.992621Z",
     "start_time": "2018-11-22T20:07:35.867Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sm = SMOTE(random_state=42) #this takes a long time so i would suggest saving to pkl and loading the pkl\n",
    "# x_res, y_res = sm.fit_sample(x_features, y_target)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_features, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:07:53.995031Z",
     "start_time": "2018-11-22T20:07:35.869Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:07:53.996244Z",
     "start_time": "2018-11-22T20:07:35.870Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy_classifier = DummyClassifier()\n",
    "dummy_classifier.fit(x_train, y_train)\n",
    "print(accuracy_score(dummy_classifier.predict(x_test), y_test))\n",
    "confusion_matrix(dummy_classifier.predict(x_test), y_test)\n",
    "# scaler = StandardScaler()\n",
    "# x_features = scaler.fit_transform(x_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because it's an imbalanced dataset lmao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:07:53.997898Z",
     "start_time": "2018-11-22T20:07:35.873Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()\n",
    "# selector = RFE(lr_model, 10) # this takes hella long to run nevermind\n",
    "lr_model.fit(x_train, y_train)\n",
    "accuracy_score(lr_model.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:07:53.998840Z",
     "start_time": "2018-11-22T20:07:35.875Z"
    }
   },
   "outputs": [],
   "source": [
    "Counter(lr_model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:07:54.000770Z",
     "start_time": "2018-11-22T20:07:35.877Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sorted(list(zip(lr_model.coef_[0], x_features.columns)), key= lambda x: abs(x[0]), reverse = True)\n",
    "lr_coef_df = pd.DataFrame(list(zip(lr_model.coef_[0], x_features.columns)))\n",
    "lr_coef_df['coef_abs'] = abs(lr_coef_df.iloc[:,0])\n",
    "lr_coef_df.sort_values('coef_abs', ascending= False, inplace = True)\n",
    "lr_coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:07:54.003819Z",
     "start_time": "2018-11-22T20:07:35.879Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I would be interested to see how well i can predict if i just had the skip_3 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:07:54.007023Z",
     "start_time": "2018-11-22T20:07:35.880Z"
    }
   },
   "outputs": [],
   "source": [
    "skip_3_lr = LogisticRegression()\n",
    "skip_3_lr.fit(x_train[['skip_3']], y_train)\n",
    "accuracy_score(skip_3_lr.predict(x_test[['skip_3']]), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T20:07:54.008871Z",
     "start_time": "2018-11-22T20:07:35.883Z"
    }
   },
   "outputs": [],
   "source": [
    "(confusion_matrix(skip_3_lr.predict(x_test[['skip_3']]), y_test))\n",
    "# why is that even with just this one variable, i am able to get 98% accuracy..\n",
    "\n",
    "# recall that skip_3 indicates that if false, most of the song was played"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ya know, i never did explore if premium users are different from not-premium users. i would be interested to see if there is clustering possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# i should create a column that tells me how many songs was skipped before the current song in the session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
